# 2/16 Week log

# 2/16
- Set up remote ssh using tailscale for desktop
- Moved from directly using isaac sim to using isaac lab
- Trained model on Ant and Cube Lift tasks using example configurations

# 2/18
- There are no example rewards for cube stacking we will need to create a new reward function for that.
- Lift Observations:
	- joint pos, vel
	- object pos
	- target object pos
	- previous action taken (why do we need this input?)
- Example rewards for lift include:
	- Distance to object
	- Lifting object
	- Distance to goal, once object is lifted
- Stack Progrssion:
	- Start with just stacking 1 cube.
		- Write a reward function to get it working with current observations
	- Use pretrained VLA and measure preformance
	- Shift to camera / vision based policy
		- Potentially get this working with Lift first

- Proposed rewards for stack (1): (Note very perscriptive to start with dense rewards)
	- Distance to cube 2
	- Cube height up to height for stack
	- Distance to goal once lifted
	- Release cube once stacked.

- Even before really starting I can see the issues with keeping the policy sufficiently knowledgable to have reasonable rollouts.
	- Dense supervision just trying to give policy signal at all steps.
- Provide just initial state and goal state:  (downside is that it still requires some dataset of demonstration states, but potentially could be a much smaller dataset)
	- Sample initial state and goal state from demonstation rollouts along task.
	- Progress the time gap between initial states and final states. (curriculum)
	
